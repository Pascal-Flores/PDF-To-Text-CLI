<article>
	<preamble>Torres.pdf</preamble>
	<title>Summary Evaluationwith and without References</title>
	<authors>not implemented yet</authors>
	<abstract>Abstract—We study a new content-based method for the evaluation of text summarization systems without human models which is used to produce system rankings. The research is carried out using a new content-based evaluation framework called FRESA to compute a variety of divergences among probability distributions. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as COVERAGE, RESPONSIVENESS, PYRAMIDS and ROUGE studying their associations in various text summarization tasks including generic multi-document summarization in English and French, focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. Index Terms—Text summarization evaluation, content-based evaluation measures, divergences.</abstract>
	<reference>
15 Polibits (42) 2010
– J S summarizer, a summarization system that scores
and ranks sentences according to their Jensen-Shannon
divergence to the source document;
– a lead-based summarization system that selects the lead
sentences of the document;
– a random-based summarization system that selects
sentences at random;
– Open Text Summarizer [23], a multi-lingual summarizer
based on the frequency and
– commercial systems: Word, SSSummarizer8
, Pertinence9
and Copernic10
.
C. Evaluation Measures
The following measures derived from human assessment of
the content of the summaries are used in our experiments:
– COVERAGE is understood as the degree to which one
peer summary conveys the same information as a model
summary [2]. COVERAGE was used in DUC evaluations.
This measure is used as indicated in equation 3 using
human references or models.
– RESPONSIVENESS ranks summaries in a 5-point scale
indicating how well the summary satisfied a given
information need [2]. It is used in focused-based
summarization tasks. This measure is used as indicated
in equation 4 since a human judges the summary
with respect to a given input “user need” (e.g., a
question). RESPONSIVENESS was used in DUC and TAC
evaluations.
– PYRAMIDS [11] is a content assessment measure which
compares content units in a peer summary to weighted
content units in a set of model summaries. This
measure is used as indicated in equation 3 using human
references or models. PYRAMIDS is the adopted metric
for content-based evaluation in the TAC evaluations.
For DUC and TAC datasets the values of these measures are
available and we used them directly. We used the following
automatic evaluation measures in our experiments:
– ROUGE [14], which is a recall metric that takes into
account n-grams as units of content for comparing peer
and model summaries. The ROUGE formula specified in
[10] is as follows:
ROUGE-n(R, M) =
P
m ∈ M
P
n−gram∈P countmatch(n − gram)
P
m ∈ M
P
count(n-gram)
(5)
where R is the summary to be evaluated, M is the set of
model (human) summaries, countmatch is the number of
common n-grams in m and P, and count is the number
of n-grams in the model summaries. For the experiments
8http://www.kryltech.com/summarizer.htm
9http://www.pertinence.net
10http://www.copernic.com/en/products/summarizer
presented here we used uni-grams, 2-grams, and the skip
2-grams with maximum skip distance of 4 (ROUGE-1,
ROUGE-2 and ROUGE-SU4). ROUGE is used to compare
a peer summary to a set of model summaries in our
framework (as indicated in equation 3).
– Jensen-Shannon divergence formula given in Equation 2
is implemented in our FRESA package with the following
specification (Equation 6) for the probability distribution
of words w.
Pw =
CT
w
N
Qw =
(
CS
w
NS
if w ∈ S
CT
w +δ
N+δ∗B otherwise
(6)
Where P is the probability distribution of words w in
text T and Q is the probability distribution of words w
in summary S; N is the number of words in text and
summary N = NT +NS, B = 1.5|V |, CT
w is the number
of words in the text and CS
w is the number of words in
the summary. For smoothing the summary’s probabilities
we have used δ = 0.005. We have also implemented
other smoothing approaches (e.g. Good-Turing [24], that
uses the CPAN Perl’s Statistics-Smoothing-SGT-2.1.2
package11
) in FRESA, but we do not use them in
the experiments reported here. Following the ROUGE
approach, in addition to word uni-grams we use 2-grams
and skip n-grams computing divergences such as J S
(using uni-grams) J S2 (using 2-grams), J S4 (using the
skip n-grams of ROUGE-SU4), and J SM which is an
average of the J Si. J Ss measures are used to compare a
peer summary to its source document(s) in our framework
(as indicated in equation 4). In the case of summarization
of multiple documents, these are concatenated (in the
given input order) to form a single input from which
probabilities are computed.
IV. EXPERIMENTS AND RESULTS
We first replicated the experiments presented in [12] to
verify that our implementation of J S produced correlation
results compatible with that work. We used the TAC’08
Update Summarization data set and computed J S and
ROUGE measures for each peer summary. We produced
two system rankings (one for each measure), which were
compared to rankings produced using the manual PYRAMIDS
and RESPONSIVENESS scores. Spearman correlations were
computed among the different rankings. The results are
presented in Table I. These results confirm a high correlation
among PYRAMIDS, RESPONSIVENESS and J S. We also
verified high correlation between J S and ROUGE-2 (0.83
Spearman correlation, not shown in the table) in this task and
dataset.
Then, we experimented with data from DUC’04, TAC’08
Opinion Summarization pilot task as well as single and
11http://search.cpan.org/∼bjoernw/Statistics-Smoothing-SGT-2.1.2/
Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales
16
Polibits (42) 2010
TABLE I
SPEARMAN CORRELATION OF CONTENT-BASED MEASURES IN TAC’08
UPDATE SUMMARIZATION TASK
Mesure PYRAMIDS p-value RESPONSIVENESS p-value
ROUGE-2 0.96 p < 0.005 0.92 p < 0.005
J S 0.85 p < 0.005 0.74 p < 0.005
multi-document summarization in Spanish and French. In spite
of the fact that the experiments for French and Spanish corpora
use less data points (i.e., less summarizers per task) than
for English, results are still quite significant. For DUC’04,
we computed the J S measure for each peer summary in
tasks 2 and 5 and we used J S, ROUGE, COVERAGE and
RESPONSIVENESS scores to produce systems’ rankings. The
various Spearman’s rank correlation values for DUC’04 are
presented in Tables II (for task 2) and III (for task 5).
For task 2, we have verified a strong correlation between
J S and COVERAGE. For task 5, the correlation between
J S and COVERAGE is weak, and that between J S and
RESPONSIVENESS is weak and negative.
Although the Opinion Summarization (OS) task is a new
type of summarization task and its evaluation is a complicated
issue, we have decided to compare J S rankings with those
obtained using PYRAMIDS and RESPONSIVENESS in TAC’08.
Spearman’s correlation values are listed in Table IV. As it can
be seen, there is weak and negative correlation of J S with
both PYRAMIDS and RESPONSIVENESS. Correlation between
PYRAMIDS and RESPONSIVENESS rankings is high for this
task (0.71 Spearman’s correlation value).
For experimentation in mono-document summarization
in Spanish and French, we have run 11 multi-lingual
summarization systems; for experimentation in French, we
have run 12 systems. In both cases, we have produced
summaries at a compression rate close to the compression rate
of the authors’ provided abstracts. We have then computed J S
and ROUGE measures for each summary and we have averaged
the measure’s values for each system. These averages were
used to produce rankings per each measure. We computed
Spearman’s correlations for all pairs of rankings.
Results are presented in Tables V, VI and VII. All results
show medium to strong correlation between the J S measures
and ROUGE measures. However the J S measure based on
uni-grams has lower correlation than J Ss which use n-grams
of higher order. Note that table VII presents results for
generic multi-document summarization in French, in this
case correlation scores are lower than correlation scores for
single-document summarization in French, a result which may
be expected given the diversity of input in multi-document
summarization.
V. DISCUSSION
The departing point for our inquiry into text summarization
evaluation has been recent work on the use of content-based
evaluation metrics that do not rely on human models but that
compare summary content to input content directly [12]. We
have some positive and some negative results regarding the
direct use of the full document in content-based evaluation.
We have verified that in both generic muti-document
summarization and in topic-based multi-document
summarization in English correlation among measures
that use human models (PYRAMIDS, RESPONSIVENESS
and ROUGE) and a measure that does not use models
(J S divergence) is strong. We have found that correlation
among the same measures is weak for summarization of
biographical information and summarization of opinions in
blogs. We believe that in these cases content-based measures
should be considered, in addition to the input document, the
summarization task (i.e. text-based representation, description)
to better assess the content of the peers [25], the task being a
determinant factor in the selection of content for the summary.
Our multi-lingual experiments in generic single-document
summarization confirm a strong correlation among the
J S divergence and ROUGE measures. It is worth noting
that ROUGE is in general the chosen framework for
presenting content-based evaluation results in non-English
summarization.
For the experiments in Spanish, we are conscious that we
only have one model summary to compare with the peers.
Nevertheless, these models are the corresponding abstracts
written by the authors. As the experiments in [26] show, the
professionals of a specialized domain (as, for example, the
medical domain) adopt similar strategies to summarize their
texts and they tend to choose roughly the same content chunks
for their summaries. Previous studies have shown that author
abstracts are able to reformulate content with fidelity [27] and
these abstracts are ideal candidates for comparison purposes.
Because of this, the summary of the author of a medical article
can be taken as reference for summaries evaluation. It is worth
noting that there is still debate on the number of models to be
used in summarization evaluation [28]. In the French corpus
PISTES, we suspect the situation is similar to the Spanish
case.
VI. CONCLUSIONS AND FUTURE WORK
This paper has presented a series of experiments in
content-based measures that do not rely on the use of model
summaries for comparison purposes. We have carried out
extensive experimentation with different summarization tasks
drawing a clearer picture of tasks where the measures could
be applied. This paper makes the following contributions:
– We have shown that if we are only interested in ranking
summarization systems according to the content of their
automatic summaries, there are tasks were models could
be subtituted by the full document in the computation of
the J S measure obtaining reliable rankings. However,
we have also found that the substitution of models
by full-documents is not always advisable. We have
Summary Evaluation with and without References
17 Polibits (42) 2010
TABLE II
SPEARMAN ρ OF CONTENT-BASED MEASURES WITH COVERAGE IN DUC’04 TASK 2
Mesure COVERAGE p-value
ROUGE-2 0.79 p < 0.0050
J S 0.68 p < 0.0025
TABLE III
SPEARMAN ρ OF CONTENT-BASED MEASURES IN DUC’04 TASK 5
Mesure COVERAGE p-value RESPONSIVENESS p-value
ROUGE-2 0.78 p < 0.001 0.44 p < 0.05
J S 0.40 p < 0.050 -0.18 p < 0.25
TABLE IV
SPEARMAN ρ OF CONTENT-BASED MEASURES IN TAC’08 OS TASK
Mesure PYRAMIDS p-value RESPONSIVENESS p-value
J S -0.13 p < 0.25 -0.14 p < 0.25
TABLE V
SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE Medicina Clı́nica CORPUS (SPANISH)
Mesure ROUGE-1 p-value ROUGE-2 p-value ROUGE-SU4 p-value
J S 0.56 p < 0.100 0.46 p < 0.100 0.45 p < 0.200
J S2 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005
J S4 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005
J SM 0.82 p < 0.005 0.71 p < 0.020 0.71 p < 0.010
found weak correlation among different rankings in
complex summarization tasks such as the summarization
of biographical information and the summarization of
opinions.
– We have also carried out large-scale experiments in
Spanish and French which show positive medium to
strong correlation among system’s ranks produced by
ROUGE and divergence measures that do not use the
model summaries.
– We have also presented a new framework, FRESA, for
the computation of measures based on J S divergence.
Following the ROUGE approach, FRESA package use
word uni-grams, 2-grams and skip n-grams computing
divergences. This framework will be available to the
community for research purposes.
Although we have made a number of contributions, this paper
leaves many open questions than need to be addressed. In
order to verify correlation between ROUGE and J S, in the
short term we intend to extend our investigation to other
languages such as Portuguese and Chinesse for which we
have access to data and summarization technology. We also
plan to apply FRESA to the rest of the DUC and TAC
summarization tasks, by using several smoothing techniques.
As a novel idea, we contemplate the possibility of adapting
the evaluation framework for the phrase compression task
[29], which, to our knowledge, does not have an efficient
evaluation measure. The main idea is to calculate J S from
an automatically-compressed sentence taking the complete
sentence by reference. In the long term, we plan to incorporate
a representation of the task/topic in the calculation of
measures. To carry out these comparisons, however, we are
dependent on the existence of references.
FRESA will also be used in the new question-answer task
campaign INEX’2010 (http://www.inex.otago.ac.nz/tracks/qa/
qa.asp) for the evaluation of long answers. This task aims
to answer a question by extraction and agglomeration of
sentences in Wikipedia. This kind of task corresponds
to those for which we have found a high correlation
among the measures J S and evaluation methods with
human intervention. Moreover, the J S calculation will be
among the summaries produced and a representative set of
relevant passages from Wikipedia. FRESA will be used to
compare three types of systems, although different tasks: the
multi-document summarizer guided by a query, the search
systems targeted information (focused IR) and the question
answering systems.
ACKNOWLEDGMENT
We are grateful to the Programa Ramón y Cajal from
Ministerio de Ciencia e Innovación, Spain. This work is
partially supported by: a postdoctoral grant from the National
Program for Mobility of Research Human Resources (National
Plan of Scientific Research, Development and Innovation
2008-2011, Ministerio de Ciencia e Innovación, Spain); the
research project CONACyT, number 82050, and the research
project PAPIIT-DGAPA (Universidad Nacional Autónoma de
México), number IN403108.
Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velázquez-Morales
18
Polibits (42) 2010
TABLE VI
SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE PISTES CORPUS (FRENCH)
Mesure ROUGE-1 p-value ROUGE-2 p-value ROUGE-SU4 p-value
J S 0.70 p < 0.050 0.73 p < 0.05 0.73 p < 0.500
J S2 0.93 p < 0.002 0.86 p < 0.01 0.86 p < 0.005
J S4 0.83 p < 0.020 0.76 p < 0.05 0.76 p < 0.050
J SM 0.88 p < 0.010 0.83 p < 0.02 0.83 p < 0.010
TABLE VII
SPEARMAN ρ OF CONTENT-BASED MEASURES WITH ROUGE IN THE RPM2 CORPUS (FRENCH)
Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-SU4 p-value
J S 0.830 p < 0.002 0.660 p < 0.05 0.741 p < 0.01
J S2 0.800 p < 0.005 0.590 p < 0.05 0.680 p < 0.02
J S4 0.750 p < 0.010 0.520 p < 0.10 0.620 p < 0.05
J SM 0.850 p < 0.002 0.640 p < 0.05 0.740 p < 0.01
REFERENCES
[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and
B. Sundheim, “Summac: a text summarization evaluation,” Natural
Language Engineering, vol. 8, no. 1, pp. 43–68, 2002.
[2] P. Over, H. Dang, and D. Harman, “DUC in context,” IPM, vol. 43,
no. 6, pp. 1506–1520, 2007.
[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,
USA: NIST, November 17-19 2008.
[4] K. Spärck Jones and J. Galliers, Evaluating Natural Language
Processing Systems, An Analysis and Review, ser. Lecture Notes in
Computer Science. Springer, 1996, vol. 1083.
[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, “A comparison of
rankings produced by summarization evaluation measures,” in NAACL
Workshop on Automatic Summarization, 2000, pp. 69–78.
[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, “Meta-evaluation
of Summaries in a Cross-lingual Environment using Content-based
Metrics,” in COLING 2002, Taipei, Taiwan, August 2002, pp. 849–855.
[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. Çelebi,
D. Liu, and E. Drábek, “Evaluation challenges in large-scale document
summarization,” in ACL’03, 2003, pp. 375–382.
[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, “BLEU: a method
for automatic evaluation of machine translation,” in ACL’02, 2002, pp.
311–318.
[9] K. Pastra and H. Saggion, “Colouring summaries BLEU,” in Evaluation
Initiatives in Natural Language Processing. Budapest, Hungary: EACL,
14 April 2003.
[10] C.-Y. Lin, “ROUGE: A Package for Automatic Evaluation of
Summaries,” in Text Summarization Branches Out: ACL-04 Workshop,
M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74–81.
[11] A. Nenkova and R. J. Passonneau, “Evaluating Content Selection in
Summarization: The Pyramid Method,” in HLT-NAACL, 2004, pp.
145–152.
[12] A. Louis and A. Nenkova, “Automatically Evaluating Content Selection
in Summarization without Human Models,” in Empirical Methods in
Natural Language Processing, Singapore, August 2009, pp. 306–314.
[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032
[13] J. Lin, “Divergence Measures based on the Shannon Entropy,” IEEE
Transactions on Information Theory, vol. 37, no. 145-151, 1991.
[14] C.-Y. Lin and E. Hovy, “Automatic Evaluation of Summaries Using
N-gram Co-occurrence Statistics,” in HLT-NAACL. Morristown, NJ,
USA: Association for Computational Linguistics, 2003, pp. 71–78.
[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, “An information-theoretic
approach to automatic evaluation of summaries,” in HLT-NAACL,
Morristown, USA, 2006, pp. 463–470.
[16] S. Kullback and R. Leibler, “On information and sufficiency,” Ann. of
Math. Stat., vol. 22, no. 1, pp. 79–86, 1951.
[17] S. Siegel and N. Castellan, Nonparametric Statistics for the Behavioral
Sciences. McGraw-Hill, 1998.
[18] C. de Loupy, M. Guégan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,
“A French Human Reference Corpus for multi-documents
summarization and sentence compression,” in LREC’10, vol. 2,
Malta, 2010, p. In press.
[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, “Textual Energy
of Associative Memories: performants applications of Enertex algorithm
in text summarization and topic segmentation,” in MICAI’07, 2007, pp.
861–871.
[20] J.-M. Torres-Moreno, P. Velázquez-Morales, and J.-G. Meunier,
“Condensés de textes par des méthodes numériques,” in JADT’02, vol. 2,
St Malo, France, 2002, pp. 723–734.
[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velázquez-Morales,
“Automatic summarization using terminological and semantic
resources,” in LREC’10, vol. 2, Malta, 2010, p. In press.
[22] J.-M. Torres-Moreno and J. Ramirez, “REG : un algorithme glouton
appliqué au résumé automatique de texte,” in JADT’10. Rome, 2010,
p. In press.
[23] V. Yatsko and T. Vishnyakov, “A method for evaluating modern
systems of automatic text summarization,” Automatic Documentation
and Mathematical Linguistics, vol. 41, no. 3, pp. 93–103, 2007.
[24] C. D. Manning and H. Schütze, Foundations of Statistical Natural
Language Processing. Cambridge, Massachusetts: The MIT Press,
1999.
[25] K. Spärck Jones, “Automatic summarising: The state of the art,” IPM,
vol. 43, no. 6, pp. 1449–1481, 2007.
[26] I. da Cunha, L. Wanner, and M. T. Cabré, “Summarization of specialized
discourse: The case of medical articles in spanish,” Terminology, vol. 13,
no. 2, pp. 249–286, 2007.
[27] C.-K. Chuah, “Types of lexical substitution in abstracting,” in ACL
Student Research Workshop. Toulouse, France: Association for
Computational Linguistics, 9-11 July 2001 2001, pp. 49–54.
[28] K. Owkzarzak and H. T. Dang, “Evaluation of automatic summaries:
Metrics under varying data conditions,” in UCNLG+Sum’09, Suntec,
Singapore, August 2009, pp. 23–30.
[29] K. Knight and D. Marcu, “Statistics-based summarization-step one:
Sentence compression,” in Proceedings of the National Conference on
Artificial Intelligence. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999, 2000, pp. 703–710.
Summary Evaluation with and without References
19 Polibits (42) 2010
</reference>
</article>
